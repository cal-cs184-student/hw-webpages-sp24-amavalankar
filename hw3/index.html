<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Aditya Mavalankar, Jackson Gold</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-amavalankar/hw3/index.html">HERE</a></h2>

<br><br>
<div>

<h2 align="middle">Overview</h2>
<p>
  The project journey unfolds with the crafting of a ray tracing engine, implementing the Moller-Trumbore algorithm for precise ray-triangle intersections. It then ventures into optimizing the rendering of intricate scenes via BVH acceleration, significantly cutting down computation time. Delving into the realm of lighting, it explores the nuances of direct and indirect illumination, adopting both uniform hemisphere and targeted light sampling to refine shadow softness and noise. The indirect lighting technique, enhanced with Russian roulette for recursion termination, captures global illumination effects. Finally, it introduces adaptive sampling, smartly adjusting sample rates to balance detail and efficiency across varying scene complexities, culminating in high-quality, nuanced visual outcomes.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  In the ray tracing rendering pipeline, rays are generated by mapping each pixel through the camera's field of view to the scene, creating a path from the camera to the pixel's corresponding point in world space. These rays are then tested for intersections with scene primitives, identifying the closest hit, if any. At these intersection points, the color and shading are computed based on material properties and lighting, determining the final color of each pixel in the rendered image.

</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  The triangle intersection algorithm uses the Moller-Trumbore method, which calculates if and where a ray intersects a triangle by:
    <li>Finding two edges of the triangle from its vertices.</li>
    <li>Using the ray&#39;s direction to calculate a determinant that tells us if the ray is parallel to the triangle&#39;s plane.</li>
    <li>If not parallel, it computes barycentric coordinates to determine if the intersection is within the triangle&#39;s bounds.</li>
    <li>It then calculates the distance along the ray to the intersection point.</li>
    <li>If this distance is within the ray&#39;s valid range, an intersection occurs.</li>
    <li>For an actual intersection, the algorithm also computes the surface normal at the intersection and updates the relevant data for rendering.</li>
    </ul>
    </li>
    </ul>
    
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p1_banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae example</figcaption>
      </td>
      <td>
        <img src="images/p1_building.png" align="middle" width="400px"/>
        <figcaption>buildig.dae example</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  <p>The BVH construction algorithm uses a recursive method that:</p>
  <ul>
  <li>Calculates a bounding box for the current set of primitives.</li>
  <li>Checks for leaf condition: If the count is less than or equal to a set threshold, it creates a leaf node.</li>
  <li>Selects a splitting axis based on the axis with the widest spread in primitive centroids.</li>
  <li>Determines a splitting point using the median of the centroid positions along the selected axis.</li>
  <li>Partitions primitives into two groups around the median.</li>
  <li>Recurses to build sub-trees for each group, creating interior nodes that link to these sub-trees.
  The heuristic is choosing the median point along the axis of greatest length, aiming for a balanced distribution of primitives across nodes.</li>
  </ul>
  
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p2_cblucy.png" align="middle" width="400px"/>
        <figcaption>cblucy.dae example</figcaption>
      </td>
      <td>
        <img src="images/p2_maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplank.dae example</figcaption>
      </td>
    </tr>
    <tr align="center">
     
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  Using BVH acceleration, rendering times for complex scenes are significantly reduced. For example, a detailed cow model that takes about 10 seconds to render without BVH is rendered almost instantaneously with it. Similarly, a complex building scene goes from taking several minutes to render to being completed almost instantly when BVH is utilized. These examples clearly demonstrate the effectiveness of BVH in optimizing rendering by quickly eliminating irrelevant geometry from intersection tests. I do, however, realize that I must have a more powerful computer than the hives, as it took nowhere near 40 minutes to generate. You can see in part 1, the images that were rendered.
</p>
<br>


<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  <ul>
    <li><p>Uniform hemisphere sampling:</p>
    <ul>
    <li>We use a Monte Carlo approach to estimate the indirect lighting on a a surface through multiple samples.</li>
    <li>We first initialize the variables wi_d and the density function pdf to store the sample direction and probability of a sample, respectively.</li>
    <li>We sample num_samples times. For each sample, we sample a direction wi_d from the BSDF function at the intersection point provided.</li>
    <li>As mentioned in the project spec, we found the cos of the angle between the sampled direction, wi_d and the surface normal := Vector3D(0, 0, 1)</li>
    <li>Then, we transform the sampled direction into more meaningful world coordinates ub next_d and next_o represents the origin of the new sampled ray.</li>
    <li>We check that the new ray intersects the scene, and that the cosine of the angle is positive because we only care about light paths originating above the surface.</li>
    <li>We accumulate the samples; in accumulation, we divide it by the pdf, or the probability we selected that sample.</li>
    <li>Finally, we divide the accumulated L_out by the number of samples and return that value.</li>
    </ul>
    </li>
    <li><p>Direct lighting importance:</p>
    <ul>
    <li>We iterate over all the lights in the scene to compute its contribution to the radiance at the intersection point.</li>
    <li>For every light, we perform several samples to estimate its respective radiance</li>
    <li>We sample radiance, L, from the light source at the point of intersection, hit_p and like in the other method, we compute the cosine of the angle between the simple direction and intersection point.</li>
    <li>We check for a positive cosine (for the same reason as above) and also that the sampled ray does not intersect any objects in our scene. Assuming both partners are met, we calculate the reflection coefficient of the surface and accumulate it into our total radiance, of course, divided by the pdf or the probability with which we selected the ray.</li>
    <li>We have a running counter of all the samples, which goes up every time we accumulate another ray; at the end, we divide L_out by this counter to normalize the output.</li>
    </ul>
    </li>
    </ul>    
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/p3/p3_bunny_hemi.png" align="middle" width="400px"/>
      </td>
      <td>
        <img src="images/p3/p3_bunny_importance.png" align="middle" width="400px"/>

      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/p3/p3_spheres_hemi.png" align="middle" width="400px"/>

      </td>
      <td>
        <img src="images/p3/p3_spheres_importance.png" align="middle" width="400px"/>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p3/p3_light_ray_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/p3/p3_light_ray_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p3/p3_light_ray_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/p3/p3_light_ray_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  We see that as we increase the number of rays, the noise in the samples, especially with respect to the shadows, decreases dramatically—yielding a much clearer image.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  In comparing lighting sampling to uniform hemisphere sampling, it becomes evident that lighting sampling, which directly samples from light sources, outperforms the latter. The direct approach of lighting sampling results in cleaner, clearer, and more realistic renderings compared to the random direction selection of uniform hemisphere sampling. This distinction highlights the advantages of accounting for the distribution of light sources in achieving superior lighting effects.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  <ul>
    <li>Our implementation required recursively calling itself.&#39;</li>
    <li>First, we do the coordinate space transformation to ensure we can handle the proper lighting computations regardless of the orientation of the surface.</li>
    <li>We init a few variables, specifically, the Ray r_n, and n_d and n_o, which represent the direction and origin of the newly calculated ray, respectively.</li>
    <li>Since we use Russian roulette termination, we have our probability, in this case 0.45, with which we terminate the recursive iteration.</li>
    <li>If we choose not to terminate, then we compute a sample and add it to the output Vector3D.</li>
    <li>This is identical to the direct lighting function, described in the previous section.</li>
    <li>In addition, we also reduce the next Ray’s depth by 1, and in the recursive call, check if our depth is greater than 1. If it is, we continue recursively iterating. If it’s not, we terminate.</li>
    <li>In our recursive call, we also divide by (pdf and cpdf=0.45) to compensate for the fact that many paths are terminated at-chance</li>
    <li>Finally, we also check if we want to accumulate the bounces or not based off the isAccumBounces variable. If it is true, then we add the value from each recursive call. If it is false, we simply take the value of the last call.</li>
    </ul>
    
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/p4_bench_direct.png" align="middle" width="400px"/>
        <figcaption>bench direct</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_bench_indirect.png" align="middle" width="400px"/>
        <figcaption>bench indirect</figcaption>
      </td>
    </tr>

    <tr align="center">
      <td>
        <img src="images/p4/p4_dragon_direct.png" align="middle" width="400px"/>
        <figcaption>dragon direct</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_dragon_indirect.png" align="middle" width="400px"/>
        <figcaption>dragon indirect</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/p4_only_direct.png" align="middle" width="400px"/>
        <figcaption>direct illumination</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_only_indirect.png" align="middle" width="400px"/>
        <figcaption>indirect illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<br>

<h3>For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 100 (the -m flag), and isAccumBounces=false. Explain in your writeup what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>
  In observing the 2nd bounce of light (max_ray_depth=1), we witness indirect illumination, softening shadows and adding depth to the scene. With the 3rd bounce (max_ray_depth=2), further diffusion and scattering create smoother transitions between light and dark areas, contributing significantly to the image's realism compared to rasterization techniques.
</p>

<br>


<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_acc_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_acc_1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_acc_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_acc_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_acc_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  It's clear that as we increase the max ray depth, the image gets brighter due to the rays bouncing more and more. When we have a high ray depth, however, we have the possibility for higher noise due to the potential variance in the Monte Carlo sampling.
</p>
<br>

<h3>For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100 (the -m flag). Use 1024 samples per pixel.</h3>
<br>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/P4_CBunny_RR_RR_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p4/p4_sample_rate_1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_sample_rate_2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/p4_sample_rate_4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_sample_rate_8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/p4_sample_rate_16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/p4/p4_sample_rate_64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p4/p4_sample_rate_1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  We see, unsurprisingly, that as we increase the number of samples, the quality of the image grows immensely, notably, that noise is reduced. This is because the more samples we conduct, the closer to reality the output image will look like.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  <ul>
    <li>Adaptive sampling is when we increase or decrease the number of samples for a point given its relative complexity, with a goal of saving time by requiring fewer samples for less complicated elements of a scene. Though there are many ways to implement adaptive sampling, we make use of the principle of variance; a more complicated pixel will have more variance than a simpler one across several samples. Thus, every time we reach the number of samples threshold, we check to see how much variance there is within the samples, utilizing principles of statistics and the formulas provided in the project spec</li>
    <li>Implementation:<ul>
    <li>It builds off the implementation of the ray generation in part 1. In our for loop for the samples, we have a separate counter titled sampleCount. Every time that hits the number of samples per batch, we calculate <code>I</code> and check to see if it is under our variance threshold (as defined in project spec) If it is, that means we can reasonably assume we’ve converged, allowing us to break from the loop and return the Ray.</li>
    <li>Until then, every time we sample, we add the illuminance and illuminance^2 to s1 and s2, respectively.</li>
    <li>Finally, when we return the average radiance, we divide by the aforementioned counter, instead of the original num_samples.</li>
    </ul>
    </li>
    </ul>
    
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/p5/adaptive_one.png" align="middle" width="400px"/>
        <figcaption>Rendered image (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="images/p5/adaptive_one_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (wall-e.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/p5/adaptive_two.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/p5/adaptive_two_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2>Working Together + Collaboration</h2>
<p>Working on this homework together, we tackled all the problems side by side because each part built on the last. Although we did split up tasks within those parts sometimes, doing so played to our strengths and kept us both in the loop. This approach made things more efficient and helped us learn a lot, especially about the importance of good communication and being flexible with each other’s working styles. In the end, it was a great mix of teamwork and individual effort, teaching us not just about the homework topics, but also how to work well together.</p>
<br>

</body>
</html>
